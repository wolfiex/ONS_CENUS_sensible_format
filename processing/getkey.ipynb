{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install beautifulsoup4 selenium webdriver-manager\n",
    "\n",
    "# import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# headless browser\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# read zipfile in memory\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# parsing \n",
    "import pandas as pd\n",
    "import p_tqdm as tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section we use the chrome webbrowser to get a list of all available links.\n",
    "\n",
    "- We need to do this rather than use a request, as the page structure is populated by javascript and not the html source code itself. \n",
    "- This means we open the browser, load the code, get the source and close it again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: \n",
      "\n",
      "../DATA/OA11/: File exists\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 98.0.4758\n",
      "Get LATEST chromedriver version for 98.0.4758 google-chrome\n",
      "Driver [/Users/danielellis/.wdm/drivers/chromedriver/mac64/98.0.4758.80/chromedriver] found in cache\n",
      "/var/folders/h8/t803dxss41g9kj07lkgx95fm0000gn/T/ipykernel_71523/3262914912.py:12: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://www.nomisweb.co.uk/census/2011/bulk/r2_2#KeyStatistics\"\n",
    "domain = 'https://www.nomisweb.co.uk/'\n",
    "loc = '../DATA/'\n",
    "prefix = 'OA11/'\n",
    "os.system('mkdir '+loc+prefix)\n",
    "\n",
    "#  lets create a fake headless browser\n",
    "\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n",
    "driver.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.find(\"table\", {\"class\": \"table table-striped table-bordered table-condensed\"})\n",
    "table = soup.find('tbody')\n",
    "table = list(filter(lambda x: x!= '\\n',table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the source code, we filter out the links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a id=\"KeyStatistics\" name=\"KeyStatistics\"></a>\n",
      "<a id=\"QuickStatistics\" name=\"QuickStatistics\"></a>\n"
     ]
    }
   ],
   "source": [
    "level = 0 \n",
    "filelist = []\n",
    "\n",
    "\n",
    "for i in table: \n",
    "    # // filter to key statistics only\n",
    "    if len(i)<2: \n",
    "        print(i.find('a')) ; level += 1\n",
    "        if level >1 : break\n",
    "        continue\n",
    "    \n",
    "    select = i.findAll('td')\n",
    "    if '-' in select[1].text:\n",
    "        continue # skip male female breakdown. \n",
    "    if 'Welsh' in select[1].text:\n",
    "        continue # skip welsh language skills (too specific). \n",
    "\n",
    "    obj = dict(code = select[0].text, name = select[1].text, url = select[3].find('a').get('href'))\n",
    "    filelist.append(obj)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'code': 'KS604EW',\n",
       "  'name': 'Hours Worked',\n",
       "  'url': '/output/census/2011/ks604ew_2011_oa.zip'},\n",
       " {'code': 'KS605EW',\n",
       "  'name': 'Industry',\n",
       "  'url': '/output/census/2011/ks605ew_2011_oa.zip'},\n",
       " {'code': 'KS608EW',\n",
       "  'name': 'Occupation',\n",
       "  'url': '/output/census/2011/ks608ew_2011_oa.zip'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check the last fiew values\n",
    "filelist[-3:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next step lies extracting the data:\n",
    "1. Download zip into memory \n",
    "2. Extract whist still in RAM\n",
    "3. Concatenate tables in Pandas\n",
    "4. Use description table to give this useful names. \n",
    "5. Save to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# indicator = filelist[3]\n",
    "def extract (indicator):\n",
    "    ''' A function to extract the data into memory, process it and, and then save into csvs. '''\n",
    "    try: \n",
    "        resp = urlopen(domain+indicator['url'])\n",
    "        zipfile = ZipFile(BytesIO(resp.read()))\n",
    "    except: # occasionally an in-memory download may fail. Trigger a restart\n",
    "        print('\\n\\n---',indicator)\n",
    "        try:\n",
    "            resp = urlopen(domain+indicator['url'])\n",
    "            zipfile = ZipFile(BytesIO(resp.read()))\n",
    "        except:\n",
    "            print('\\n\\n\\nFailed',indicator)\n",
    "            return\n",
    "\n",
    "    # lets read all the data into a single df\n",
    "    namelist = filter(lambda x: 'DATA' in x , zipfile.namelist())\n",
    "    all_d = pd.concat([pd.read_csv(zipfile.open(f)) for f in namelist], axis=0, ignore_index=True).set_index('GeographyCode')\n",
    "\n",
    "\n",
    "    #  Exctract Headers from the DESCRIPTION file\n",
    "    headers = pd.read_csv(zipfile.open(filter(lambda x: 'DESC' in x , zipfile.namelist()).__next__())).set_index('ColumnVariableCode')\n",
    "    headers  = headers[headers.ColumnVariableMeasurementUnit=='Count']['ColumnVariableDescription'].to_dict() #  filter to counts only\n",
    "\n",
    "    # only take columns we have count values for\n",
    "    all_d = all_d[[k for k in all_d.columns if (k in headers)]] \n",
    "    # rename columns\n",
    "    all_d.columns = [headers[k] for k in all_d.columns]\n",
    "    # we do not want summed counts as they ruin percentages\n",
    "    for c in filter(lambda x: x[:14]=='All categories',all_d.columns): all_d.drop(c,inplace=True,axis=1)\n",
    "\n",
    "    all_d.to_csv(loc+prefix+indicator['code']+'_'+indicator['name'].replace(\" \",\"_\").lower()+'.csv')\n",
    "\n",
    "    # print(indicator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual we can run this in parallel as it is is an 'embarassingly parallel' problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [03:18<00:00,  8.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- {'code': 'KS102EW', 'name': 'Age Structure', 'url': '/output/census/2011/ks102ew_2011_oa.zip'}\n",
      "\n",
      "\n",
      "--- {'code': 'KS201EW', 'name': 'Ethnic Group', 'url': '/output/census/2011/ks201ew_2011_oa.zip'}\n"
     ]
    }
   ],
   "source": [
    "_ = tqdm.p_map(extract,filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9da6e78936f6a8ab907bad03d2fac666a6d46a2192eea18a6af8d3db83b62898"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('DataServer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
